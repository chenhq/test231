{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity: test_hs300_model03\n",
      "Attempt to resume a past training if it exists:\n",
      "Starting from scratch: new trials.\n",
      "time: 2018-01-12 15:08:45.271886, identity: 6dba7718-f767-11e7-b733-525400f48026, params: {'split_dates': ('2016-01-01', '2017-01-01'), 'features': {'kline': {'window': (60,)}, 'label_by_ma_price': {'quantile_list': (0, 0.5, 1), 'window': 60, 'next_ma_window': 3}, 'ma': {'window': (60,), 'ma_list': (1, 2, 3, 5, 8, 13, 21), 'price': 'close'}}, 'lstm': {'shuffle': False, 'bias_initializer': <keras.initializers.VarianceScaling object at 0x7f9233783160>, 'batch_size': 64, 'kernel_initializer': <keras.initializers.VarianceScaling object at 0x7f923377ef28>, 'activation_last': 'softmax', 'layer3': {'activation': 'tanh', 'is_BN': False, 'units': 128}, 'epochs': 20000, 'layer2': {'activation': 'tanh', 'is_BN': False, 'units': 128}, 'loss_type': 'categorical_crossentropy', 'layer1': {'activation': 'tanh', 'is_BN': False, 'units': 128}, 'dropout': 0.30000000000000004, 'lr': 0.005, 'recurrent_dropout': 0.30000000000000004, 'time_steps': 64}}\n",
      "Train on 433 samples, validate on 54 samples\n",
      "Epoch 1/20000\n",
      "384/433 [=========================>....] - ETA: 0s - loss: 1.0023 - acc: 0.4966\n",
      " log model into file ./params_select_test_hs300_model03/6dba7718-f767-11e7-b733-525400f48026/acc_0.35.h5....\n",
      "433/433 [==============================] - 3s 7ms/step - loss: 0.9668 - acc: 0.5012 - val_loss: 0.6830 - val_acc: 0.5663\n",
      "Epoch 2/20000\n",
      "384/433 [=========================>....] - ETA: 0s - loss: 0.6885 - acc: 0.5449\n",
      " log model into file ./params_select_test_hs300_model03/6dba7718-f767-11e7-b733-525400f48026/acc_0.4.h5....\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6884 - acc: 0.5449 - val_loss: 0.6820 - val_acc: 0.5744\n",
      "Epoch 3/20000\n",
      "384/433 [=========================>....] - ETA: 0s - loss: 0.6879 - acc: 0.5465\n",
      " log model into file ./params_select_test_hs300_model03/6dba7718-f767-11e7-b733-525400f48026/acc_0.45.h5....\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6878 - acc: 0.5477 - val_loss: 0.6826 - val_acc: 0.5576\n",
      "Epoch 4/20000\n",
      "384/433 [=========================>....] - ETA: 0s - loss: 0.6895 - acc: 0.5391\n",
      " log model into file ./params_select_test_hs300_model03/6dba7718-f767-11e7-b733-525400f48026/acc_0.5.h5....\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6890 - acc: 0.5401 - val_loss: 0.6814 - val_acc: 0.5674\n",
      "Epoch 5/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6889 - acc: 0.5439 - val_loss: 0.6831 - val_acc: 0.5422\n",
      "Epoch 6/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6872 - acc: 0.5473 - val_loss: 0.6831 - val_acc: 0.5686\n",
      "Epoch 7/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6904 - acc: 0.5419 - val_loss: 0.6762 - val_acc: 0.5715\n",
      "Epoch 8/20000\n",
      "384/433 [=========================>....] - ETA: 0s - loss: 0.6834 - acc: 0.5583\n",
      " log model into file ./params_select_test_hs300_model03/6dba7718-f767-11e7-b733-525400f48026/acc_0.55.h5....\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6830 - acc: 0.5598 - val_loss: 0.6776 - val_acc: 0.5654\n",
      "Epoch 9/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6826 - acc: 0.5610 - val_loss: 0.6855 - val_acc: 0.5634\n",
      "Epoch 10/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6863 - acc: 0.5479 - val_loss: 0.6842 - val_acc: 0.5625\n",
      "Epoch 11/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6815 - acc: 0.5574 - val_loss: 0.6828 - val_acc: 0.5619\n",
      "Epoch 12/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6847 - acc: 0.5515 - val_loss: 0.6805 - val_acc: 0.5654\n",
      "Epoch 13/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6781 - acc: 0.5692 - val_loss: 0.6844 - val_acc: 0.5602\n",
      "Epoch 14/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6841 - acc: 0.5605 - val_loss: 0.6832 - val_acc: 0.5579\n",
      "Epoch 15/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6769 - acc: 0.5716 - val_loss: 0.6856 - val_acc: 0.5558\n",
      "Epoch 16/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6774 - acc: 0.5705 - val_loss: 0.6830 - val_acc: 0.5689\n",
      "Epoch 17/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6803 - acc: 0.5689 - val_loss: 0.6855 - val_acc: 0.5558\n",
      "Epoch 18/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6737 - acc: 0.5818 - val_loss: 0.6940 - val_acc: 0.5451\n",
      "Epoch 19/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6760 - acc: 0.5738 - val_loss: 0.6898 - val_acc: 0.5477\n",
      "Epoch 20/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6712 - acc: 0.5820 - val_loss: 0.6891 - val_acc: 0.5475\n",
      "Epoch 21/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6741 - acc: 0.5775 - val_loss: 0.6897 - val_acc: 0.5605\n",
      "Epoch 22/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6697 - acc: 0.5856 - val_loss: 0.6984 - val_acc: 0.5532\n",
      "Epoch 23/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6722 - acc: 0.5815 - val_loss: 0.6896 - val_acc: 0.5530\n",
      "Epoch 24/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6709 - acc: 0.5837 - val_loss: 0.6915 - val_acc: 0.5573\n",
      "Epoch 25/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6689 - acc: 0.5862 - val_loss: 0.6989 - val_acc: 0.5556\n",
      "Epoch 26/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6692 - acc: 0.5892 - val_loss: 0.6925 - val_acc: 0.5535\n",
      "Epoch 27/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6647 - acc: 0.5947 - val_loss: 0.6895 - val_acc: 0.5596\n",
      "Epoch 28/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6660 - acc: 0.5937 - val_loss: 0.7086 - val_acc: 0.5434\n",
      "Epoch 29/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6632 - acc: 0.5977 - val_loss: 0.7005 - val_acc: 0.5480\n",
      "Epoch 30/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6661 - acc: 0.5929 - val_loss: 0.6972 - val_acc: 0.5584\n",
      "Epoch 31/20000\n",
      "384/433 [=========================>....] - ETA: 0s - loss: 0.6618 - acc: 0.6000\n",
      " log model into file ./params_select_test_hs300_model03/6dba7718-f767-11e7-b733-525400f48026/acc_0.6.h5....\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6608 - acc: 0.6017 - val_loss: 0.6934 - val_acc: 0.5613\n",
      "Epoch 32/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6648 - acc: 0.5964 - val_loss: 0.7064 - val_acc: 0.5388\n",
      "Epoch 33/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6607 - acc: 0.5992 - val_loss: 0.6979 - val_acc: 0.5472\n",
      "Epoch 34/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6620 - acc: 0.6002 - val_loss: 0.7093 - val_acc: 0.5492\n",
      "Epoch 35/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6589 - acc: 0.6048 - val_loss: 0.6929 - val_acc: 0.5535\n",
      "Epoch 36/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6585 - acc: 0.6011 - val_loss: 0.7048 - val_acc: 0.5582\n",
      "Epoch 37/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6554 - acc: 0.6073 - val_loss: 0.7140 - val_acc: 0.5518\n",
      "Epoch 38/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6577 - acc: 0.6092 - val_loss: 0.7133 - val_acc: 0.5428\n",
      "Epoch 39/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6581 - acc: 0.6103 - val_loss: 0.7178 - val_acc: 0.5373\n",
      "Epoch 40/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6575 - acc: 0.6074 - val_loss: 0.7141 - val_acc: 0.5431\n",
      "Epoch 41/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6553 - acc: 0.6084 - val_loss: 0.7062 - val_acc: 0.5584\n",
      "Epoch 42/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6528 - acc: 0.6132 - val_loss: 0.7129 - val_acc: 0.5486\n",
      "Epoch 43/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6555 - acc: 0.6104 - val_loss: 0.7248 - val_acc: 0.5506\n",
      "Epoch 44/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6483 - acc: 0.6179 - val_loss: 0.7270 - val_acc: 0.5440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6498 - acc: 0.6162 - val_loss: 0.7188 - val_acc: 0.5616\n",
      "Epoch 46/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6491 - acc: 0.6117 - val_loss: 0.7162 - val_acc: 0.5417\n",
      "Epoch 47/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6487 - acc: 0.6169 - val_loss: 0.7153 - val_acc: 0.5587\n",
      "Epoch 48/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6462 - acc: 0.6210 - val_loss: 0.7131 - val_acc: 0.5431\n",
      "Epoch 49/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6471 - acc: 0.6221 - val_loss: 0.7174 - val_acc: 0.5498\n",
      "Epoch 50/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6451 - acc: 0.6252 - val_loss: 0.7306 - val_acc: 0.5553\n",
      "Epoch 51/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6472 - acc: 0.6215 - val_loss: 0.7228 - val_acc: 0.5579\n",
      "Epoch 52/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6440 - acc: 0.6258 - val_loss: 0.7311 - val_acc: 0.5530\n",
      "Epoch 53/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6392 - acc: 0.6282 - val_loss: 0.7270 - val_acc: 0.5411\n",
      "Epoch 54/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6399 - acc: 0.6281 - val_loss: 0.7467 - val_acc: 0.5411\n",
      "Epoch 55/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6406 - acc: 0.6305 - val_loss: 0.7422 - val_acc: 0.5399\n",
      "Epoch 56/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6418 - acc: 0.6289 - val_loss: 0.7491 - val_acc: 0.5356\n",
      "Epoch 57/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6363 - acc: 0.6323 - val_loss: 0.7607 - val_acc: 0.5451\n",
      "Epoch 58/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6334 - acc: 0.6335 - val_loss: 0.7366 - val_acc: 0.5443\n",
      "Epoch 59/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6374 - acc: 0.6332 - val_loss: 0.7509 - val_acc: 0.5477\n",
      "Epoch 60/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6302 - acc: 0.6369 - val_loss: 0.7459 - val_acc: 0.5564\n",
      "Epoch 61/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6352 - acc: 0.6363 - val_loss: 0.7431 - val_acc: 0.5428\n",
      "Epoch 62/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6314 - acc: 0.6385 - val_loss: 0.7438 - val_acc: 0.5448\n",
      "Epoch 63/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6322 - acc: 0.6361 - val_loss: 0.7445 - val_acc: 0.5344\n",
      "Epoch 64/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6271 - acc: 0.6396 - val_loss: 0.7586 - val_acc: 0.5457\n",
      "Epoch 65/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6286 - acc: 0.6402 - val_loss: 0.7563 - val_acc: 0.5446\n",
      "Epoch 66/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6283 - acc: 0.6376 - val_loss: 0.7505 - val_acc: 0.5440\n",
      "Epoch 67/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6250 - acc: 0.6470 - val_loss: 0.7737 - val_acc: 0.5448\n",
      "Epoch 68/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6288 - acc: 0.6412 - val_loss: 0.7535 - val_acc: 0.5405\n",
      "Epoch 69/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6205 - acc: 0.6478 - val_loss: 0.7706 - val_acc: 0.5411\n",
      "Epoch 70/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6245 - acc: 0.6456 - val_loss: 0.7649 - val_acc: 0.5408\n",
      "Epoch 71/20000\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6224 - acc: 0.6483 - val_loss: 0.7634 - val_acc: 0.5408\n",
      "Epoch 72/20000\n",
      "384/433 [=========================>....] - ETA: 0s - loss: 0.6231 - acc: 0.6495\n",
      " log model into file ./params_select_test_hs300_model03/6dba7718-f767-11e7-b733-525400f48026/acc_0.65.h5....\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 0.6202 - acc: 0.6529 - val_loss: 0.7659 - val_acc: 0.5341\n",
      "59/59 [==============================] - 0s 2ms/step\n",
      "54/54 [==============================] - 0s 2ms/step\n",
      "namespace: ./params_select_test_hs300_model03/6dba7718-f767-11e7-b733-525400f48026, loss: 0.7659180583777251\n",
      "\n",
      "OPTIMIZATION STEP COMPLETE.\n",
      "\n",
      "best_params: {'split_dates': ('2016-01-01', '2017-01-01'), 'features': {'kline': {'window': (60,)}, 'label_by_ma_price': {'quantile_list': (0, 0.5, 1), 'window': 60, 'next_ma_window': 3}, 'ma': {'window': (60,), 'ma_list': (1, 2, 3, 5, 8, 13, 21), 'price': 'close'}}, 'lstm': {'shuffle': False, 'bias_initializer': <keras.initializers.VarianceScaling object at 0x7f9233783160>, 'batch_size': 64, 'kernel_initializer': <keras.initializers.VarianceScaling object at 0x7f923377ef28>, 'activation_last': 'softmax', 'layer3': {'activation': 'tanh', 'is_BN': False, 'units': 128}, 'epochs': 20000, 'layer2': {'activation': 'tanh', 'is_BN': False, 'units': 128}, 'loss_type': 'categorical_crossentropy', 'layer1': {'activation': 'tanh', 'is_BN': False, 'units': 128}, 'dropout': 0.30000000000000004, 'lr': 0.005, 'recurrent_dropout': 0.30000000000000004, 'time_steps': 64}}\n"
     ]
    }
   ],
   "source": [
    "# %load params_select.py\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "import os\n",
    "\n",
    "import seaborn as snb\n",
    "from hyperopt import hp\n",
    "from keras.initializers import glorot_uniform\n",
    "import matplotlib.pyplot as plt\n",
    "import uuid\n",
    "from data_prepare import *\n",
    "from index_components import zz500_t10\n",
    "from objective import objective\n",
    "from trial import run_a_trial\n",
    "\n",
    "snb.set()\n",
    "from loss import *\n",
    "\n",
    "try:\n",
    "    import _pickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "\n",
    "identity = 'test_hs300_model03'\n",
    "\n",
    "lstm_space = {\n",
    "    'time_steps': hp.choice('time_steps', [64]),\n",
    "    'batch_size': hp.choice('batch_size', [64]),\n",
    "    'epochs': hp.choice('epochs', [20000]),  # [100, 200, 500, 1000, 1500, 2000]\n",
    "\n",
    "    # for class\n",
    "    'activation_last': hp.choice('activation_last', ['softmax']),\n",
    "    # for regression\n",
    "    # 'activation_last': hp.choice('activation', [None, 'linear']),\n",
    "\n",
    "    #\n",
    "    'shuffle': hp.choice('shuffle', [False]),\n",
    "    'loss_type': hp.choice('loss', ['categorical_crossentropy']), #, 'weighted_categorical_crossentropy']),\n",
    "\n",
    "    'layer1': {\n",
    "        'units': hp.choice('layer1_units', [128]),\n",
    "        # 'relu', 'sigmoid', 'tanh', 'linear'\n",
    "        'activation': hp.choice('layer1_activation', ['tanh']),\n",
    "        'is_BN': hp.choice('layer1_is_BN', [False]),\n",
    "    },\n",
    "    'layer2': {\n",
    "        'units': hp.choice('layer2_units', [128]),\n",
    "        # 'relu', 'sigmoid', 'tanh', 'linear'\n",
    "        'activation': hp.choice('layer2_activation', ['tanh']),\n",
    "        'is_BN': hp.choice('layer2_is_BN', [False]),\n",
    "    },\n",
    "    'layer3': {\n",
    "        'units': hp.choice('layer3_units', [128]),\n",
    "        # 'relu', 'sigmoid', 'tanh', 'linear'\n",
    "        # Loss turns into 'nan'\n",
    "        # As far as I know, it's the combination of relu and softmax that causes numerical troubles,\n",
    "        # as relu can produce large positive values corresponding to very small probabilities.\n",
    "        # If you change your model to use, say, tanh instead of relu for the last dense layer,\n",
    "        # the problem will go away.\n",
    "        'activation': hp.choice('layer3_activation', ['tanh']),\n",
    "        'is_BN': hp.choice('layer3_is_BN', [False]),\n",
    "    },\n",
    "\n",
    "    # 'lr': hp.loguniform('lr', np.log(0.000001), np.log(0.0001)),\n",
    "    'lr': hp.choice('lr', [0.005]),\n",
    "    'dropout': hp.quniform('dropout', 0.3, 0.31, 0.1),\n",
    "    'recurrent_dropout': hp.quniform('recurrent_dropout', 0.3, 0.31, 0.1),\n",
    "    'kernel_initializer': hp.choice('kernel_initializer', [glorot_uniform(seed=123)]),\n",
    "    'bias_initializer': hp.choice('bias_initializer', [glorot_uniform(seed=456)]),\n",
    "}\n",
    "\n",
    "features_space = {\n",
    "    'kline': {\n",
    "        'window': hp.choice('kline_window', [[60]])\n",
    "    },\n",
    "    'ma': {\n",
    "        'ma_list': hp.choice('ma_list', [[1, 2, 3, 5, 8, 13, 21]]),\n",
    "        'window': hp.choice('ma_window', [[60]]),\n",
    "        'price': hp.choice('price', ['close'])\n",
    "    },\n",
    "    'label_by_ma_price': {\n",
    "        'window': hp.choice('label_window', [60]),\n",
    "        'next_ma_window': hp.choice('next_ma_window', [3]),\n",
    "        'quantile_list': hp.choice('quantile_list', [# [0, 0.1, 0.3, 0.7, 0.9, 1],\n",
    "                                                     # [0, 0.2, 0.4, 0.6, 0.8, 1],\n",
    "                                                     # [0, 0.15, 0.3, 0.7, 0.85, 1],\n",
    "                                                     # [0, 0.15, 0.35, 0.65, 0.85, 1],\n",
    "                                                     # [0, 0.3, 0.7, 1],\n",
    "                                                     # [0, 0.33, 0.66, 1],\n",
    "                                                     # [0, 0.2, 0.8, 1],\n",
    "                                                     # [0, 0.4, 0.6, 1],\n",
    "                                                     [0, 0.5, 1],\n",
    "                                                     # [0, 0.45, 1],\n",
    "                                                     # [0, 0.55, 1]\n",
    "                                                    ])\n",
    "    }\n",
    "}\n",
    "\n",
    "space = {\n",
    "    'features': features_space,\n",
    "    'lstm': lstm_space,\n",
    "    'split_dates': [\"2016-01-01\", \"2017-01-01\"]\n",
    "}\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # file_name = '../data/cs_market.csv'\n",
    "    # ohlcv_list = get_data(file_name=file_name, stks=zz500_t10)\n",
    "\n",
    "    # zz500 = pickle.load(open('../data/zz500.pkl', 'rb'))\n",
    "    # ohlcv_list = [zz500]\n",
    "\n",
    "    pickle_file = 'data/hs300_ohlcv.pkl'\n",
    "    ohlcv_list = get_pickle_data(pickle_file, [])[:20]\n",
    "\n",
    "    function = \"params_select\"\n",
    "    if identity == \"\":\n",
    "        identity = str(uuid.uuid1())\n",
    "\n",
    "    print(\"identity: {}\".format(identity))\n",
    "    namespace = function + '_' + identity\n",
    "\n",
    "    namespace = os.path.join('./', namespace)\n",
    "    if not os.path.exists(namespace):\n",
    "        os.makedirs(namespace)\n",
    "\n",
    "    with open(os.path.join(namespace, 'ohlcv_list.pkl'), 'wb') as f:\n",
    "        pickle.dump(ohlcv_list, f)\n",
    "\n",
    "    # # loss\n",
    "    # loss = 'categorical_crossentropy'\n",
    "    # loss = weighted_categorical_crossentropy5\n",
    "    objective_func = partial(objective, ohlcv_list=ohlcv_list, namespace=namespace)\n",
    "\n",
    "    trials_file = os.path.join(namespace, 'trials.pkl')\n",
    "\n",
    "    # while True:\n",
    "    run_a_trial(trials_file, objective_func, space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
